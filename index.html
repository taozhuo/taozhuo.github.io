
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Zhuo's Blog</title>
  <meta name="author" content="Zhuo Tao">

  
  <meta name="description" content="spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2 Sometimes your Spark jobs finished all stages, but Web UI hangs there forever. The &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://taozhuo.github.io/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Zhuo's Blog" type="application/atom+xml">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css" />  
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Zhuo's Blog</a></h1>
  
    <h2>Big Data, Machine Learning, Spark, MapReduce, Hadoop</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="taozhuo.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/04/10/useful-configs-for-spark-in-production/">Useful Configs for Spark in Production</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-04-10T11:07:21-07:00'><span class='date'><span class='date-month'>Apr</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>11:07 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><code>spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2</code> Sometimes your Spark jobs finished all stages, but Web UI hangs there forever. The reason is if your job generates a lot of files to commit, and <code>commitTask</code> method will rename the temporary directories before <code>commitJob</code> merges every task output file into the final output folder, this process can be slow. At first I thought I can use <code>DirectParquetOutputCommitter</code> instead of default committer. However it is removed in 2.0 because speculation might cause loss of data:  <a href="https://issues.apache.org/jira/browse/SPARK-10063">SPARK-10063</a>. Then I found the root cause is that committer algo version is set to 1 by default in Hadoop, which sets the commit to single-threaded and waits until all tasks have completed. Change the version number to 2 so that this algorithm will reduce the output commit time for large jobs by having the tasks commit directly to the final output directory as they were completing.</p>

<p><code>spark.yarn.executor.memoryOverhead=15360</code> This is the amount of OS overhead memory to be allocated to each executor. By default it&rsquo;s set to executorMemory * 0.10, but it&rsquo;s way too little when you want to shuffle large amount of data. Each task is fetching shuffle files over NIO channel. The buffers required are to be allocated from OS overheads. Give it a large number to make shuffling more stable.</p>

<p><code>spark.executor.extraJavaOptions -XX:MaxDirectMemorySize=30g</code> Direct buffer memory is located within off-heap region, usually the limit of its size is small compare to heap size. When I run jobs that write parquet files with wide and complicated schema, I often see the following error. This is due to parquet snappy codec allocates large off-heap buffers for decompression:  <a href="https://issues.apache.org/jira/browse/SPARK-4073">SPARK-4073</a>. There are two ways to fix it, one is use Lzo compression codec, the other is set a much higher off heap memory limit.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="nc">SparkException</span><span class="k">:</span> <span class="kt">Task</span> <span class="kt">failed</span> <span class="kt">while</span> <span class="kt">writing</span> <span class="kt">rows</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">execution</span><span class="o">.</span><span class="n">datasources</span><span class="o">.</span><span class="nc">DefaultWriterContainer</span><span class="o">.</span><span class="n">writeRows</span><span class="o">(</span><span class="nc">WriterContainer</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">261</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">execution</span><span class="o">.</span><span class="n">datasources</span><span class="o">.</span><span class="nc">InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="nc">InsertIntoHadoopFsRelationCommand</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">143</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">execution</span><span class="o">.</span><span class="n">datasources</span><span class="o">.</span><span class="nc">InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="nc">InsertIntoHadoopFsRelationCommand</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">143</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="nc">ResultTask</span><span class="o">.</span><span class="n">runTask</span><span class="o">(</span><span class="nc">ResultTask</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">70</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="nc">Task</span><span class="o">.</span><span class="n">run</span><span class="o">(</span><span class="nc">Task</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">86</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">executor</span><span class="o">.</span><span class="nc">Executor$TaskRunner</span><span class="o">.</span><span class="n">run</span><span class="o">(</span><span class="nc">Executor</span><span class="o">.</span><span class="n">scala</span><span class="k">:</span><span class="err">274</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">concurrent</span><span class="o">.</span><span class="nc">ThreadPoolExecutor</span><span class="o">.</span><span class="n">runWorker</span><span class="o">(</span><span class="nc">ThreadPoolExecutor</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">1145</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">concurrent</span><span class="o">.</span><span class="nc">ThreadPoolExecutor$Worker</span><span class="o">.</span><span class="n">run</span><span class="o">(</span><span class="nc">ThreadPoolExecutor</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">615</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">java</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="nc">Thread</span><span class="o">.</span><span class="n">run</span><span class="o">(</span><span class="nc">Thread</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">745</span><span class="o">)</span>
</span><span class='line'><span class="nc">Caused</span> <span class="n">by</span><span class="k">:</span> <span class="kt">java.lang.OutOfMemoryError:</span> <span class="kt">Direct</span> <span class="kt">buffer</span> <span class="kt">memory</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">java</span><span class="o">.</span><span class="n">nio</span><span class="o">.</span><span class="nc">Bits</span><span class="o">.</span><span class="n">reserveMemory</span><span class="o">(</span><span class="nc">Bits</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">658</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">java</span><span class="o">.</span><span class="n">nio</span><span class="o">.</span><span class="nc">DirectByteBuffer</span><span class="o">.&lt;</span><span class="n">init</span><span class="o">&gt;(</span><span class="nc">DirectByteBuffer</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">123</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">java</span><span class="o">.</span><span class="n">nio</span><span class="o">.</span><span class="nc">ByteBuffer</span><span class="o">.</span><span class="n">allocateDirect</span><span class="o">(</span><span class="nc">ByteBuffer</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">306</span><span class="o">)</span>
</span><span class='line'>  <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="n">codec</span><span class="o">.</span><span class="nc">SnappyCompressor</span><span class="o">.</span><span class="n">setInput</span><span class="o">(</span><span class="nc">SnappyCompressor</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">97</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>dfs.datanode.socket.write.timeout=3000000</code>, <code>dfs.socket.timeout=3000000</code>. When connection to datanode is bad, saving parquet files into HDFS sometimes gives you errors like the following. You can fix it by setting a higher datanode socket write timeout in Hadoop settings.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="nc">Suppressed</span><span class="k">:</span> <span class="kt">java.io.IOException:</span> <span class="kt">The</span> <span class="kt">file</span> <span class="kt">being</span> <span class="kt">written</span> <span class="kt">is</span> <span class="kt">in</span> <span class="kt">an</span> <span class="kt">invalid</span> <span class="kt">state.</span> <span class="kt">Probably</span> <span class="kt">caused</span> <span class="kt">by</span> <span class="kt">an</span> <span class="kt">error</span> <span class="kt">thrown</span> <span class="kt">previously.</span> <span class="kt">Current</span> <span class="kt">state:</span> <span class="kt">COLUMN</span>
</span><span class='line'> <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="nc">ParquetFileWriter$STATE</span><span class="o">.</span><span class="n">error</span><span class="o">(</span><span class="nc">ParquetFileWriter</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">146</span><span class="o">)</span>
</span><span class='line'> <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="nc">ParquetFileWriter$STATE</span><span class="o">.</span><span class="n">startBlock</span><span class="o">(</span><span class="nc">ParquetFileWriter</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">138</span><span class="o">)</span>
</span><span class='line'> <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="nc">ParquetFileWriter</span><span class="o">.</span><span class="n">startBlock</span><span class="o">(</span><span class="nc">ParquetFileWriter</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">195</span><span class="o">)</span>
</span><span class='line'> <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="nc">InternalParquetRecordWriter</span><span class="o">.</span><span class="n">flushRowGroupToStore</span><span class="o">(</span><span class="nc">InternalParquetRecordWriter</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">153</span><span class="o">)</span>
</span><span class='line'> <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="nc">InternalParquetRecordWriter</span><span class="o">.</span><span class="n">close</span><span class="o">(</span><span class="nc">InternalParquetRecordWriter</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">113</span><span class="o">)</span>
</span><span class='line'> <span class="n">at</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">parquet</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="nc">ParquetRecordWriter</span><span class="o">.</span><span class="n">close</span><span class="o">(</span><span class="nc">ParquetRecordWriter</span><span class="o">.</span><span class="n">java</span><span class="k">:</span><span class="err">112</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>There are a lot of other timeouts in Spark configs, often times we trade off some performance for higher reliability. e.g. set <code>spark.executor.heartbeatInterval=120s</code> and <code>spark.network.timeout=600s</code> if you are running different workloads such as Pig/MapReduce or ad-hoc data science jobs on a busy Yarn cluster, as they have unpredictable impact on the networking condition. Just be careful some of the timeouts follow some rules, e.g. <code>heartbeatInterval</code> should be significantly less than <code>spark.network.timeout</code>, otherwise you will see strange errors.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/02/27/how-to-tune-gradient-boosted-trees-in-spark/">How to Tune Gradient-Boosted Trees in Spark</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-02-27T14:54:30-08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>27</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>2:54 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Gradient-Boosted trees(GBT) is one of the tree ensemble models that is popular among data science community. The most popular implementation - XGBoost, is used by half of winning teams competing in Kaggle challenges. However, it requires many native libraries that do not play well with JVM-based production system such as Hadoop and Spark, adding a lot of operational complexities. Sometimes it&rsquo;s not the efficient way. E.g. in the prediction phase of the end-to-end ML pipeline, we need to apply the trained model on a large amount of data(tens of billions of records). This is done in Python via Hadoop streaming, whcih consumes a lot of memory when loading batches of data fore prediction.</p>

<p>GBT in Spark ML/Mllib. there are a lot of variants in tree ensembles in old RDD-based API and new DataFrame-based API, but both point to the same Random forest implementation in <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala">RandomForest.scala</a>. Each iteration within the main loop of <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/GradientBoostedTrees.scala">GradientBoostedTrees.scala</a> calls <code>RandomForest</code> and will stop early if some validation condition is satisfied. <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/GBTClassifier.scala">GBTClassifier.scala</a> is a wrapper class if you want to plug it into the Spark ML pipeline APIs, but it doesn&rsquo;t provide a method to output predicted probabilities. I still use the old RDD-based wrapper <a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/tree/GradientBoostedTrees.scala">GradientBoostedTrees</a> so I can calculate <code>AUPRC</code> and <code>AUROC</code> scores. Below is a code snippet on how to train the model and <code>AUPRC</code> &amp; <code>AUROC</code> calculations:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="k">val</span> <span class="n">boostingStrategy</span> <span class="k">=</span> <span class="nc">BoostingStrategy</span><span class="o">.</span><span class="n">defaultParams</span><span class="o">(</span><span class="s">&quot;Classification&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="c1">//boosting parameters</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">numIterations</span> <span class="k">=</span> <span class="mi">100</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">learningRate</span> <span class="k">=</span> <span class="mf">0.15</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">setValidationTol</span><span class="o">(</span><span class="mf">0.01</span><span class="o">)</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">numClasses</span> <span class="k">=</span> <span class="mi">2</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">categoricalFeaturesInfo</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">](</span> <span class="mi">57</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">58</span><span class="o">-&gt;</span><span class="mi">4</span><span class="o">,</span><span class="mi">59</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span><span class="mi">60</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span><span class="mi">61</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span> <span class="mi">62</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">63</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span> <span class="mi">64</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span>
</span><span class='line'>      <span class="mi">65</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">66</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span> <span class="mi">67</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span> <span class="mi">68</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">69</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span> <span class="mi">70</span><span class="o">-&gt;</span><span class="mi">2</span><span class="o">,</span> <span class="mi">72</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">73</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">74</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">93</span><span class="o">-&gt;</span><span class="mi">5</span><span class="o">,</span> <span class="mi">94</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">96</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">99</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">102</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">105</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span>
</span><span class='line'>      <span class="mi">108</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">111</span><span class="o">-&gt;</span><span class="mi">3</span><span class="o">,</span> <span class="mi">113</span><span class="o">-&gt;</span><span class="mi">10</span><span class="o">,</span> <span class="mi">114</span><span class="o">-&gt;</span><span class="mi">10</span><span class="o">)</span>
</span><span class='line'>  <span class="c1">//tree parameters</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">useNodeIdCache</span> <span class="k">=</span> <span class="kc">true</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">subsamplingRate</span> <span class="k">=</span> <span class="mf">0.8</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">maxBins</span> <span class="k">=</span> <span class="mi">10</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">maxDepth</span> <span class="k">=</span> <span class="mi">12</span>
</span><span class='line'>    <span class="n">boostingStrategy</span><span class="o">.</span><span class="n">treeStrategy</span><span class="o">.</span><span class="n">minInstancesPerNode</span> <span class="k">=</span> <span class="mi">32</span>
</span><span class='line'>    <span class="nc">Logger</span><span class="o">.</span><span class="n">getLogger</span><span class="o">(</span><span class="nc">GradientBoostedTrees</span><span class="o">.</span><span class="n">getClass</span><span class="o">).</span><span class="n">setLevel</span><span class="o">(</span><span class="nc">Level</span><span class="o">.</span><span class="nc">DEBUG</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">model</span> <span class="k">=</span> <span class="nc">GradientBoostedTrees</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">train</span><span class="o">,</span> <span class="n">boostingStrategy</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">gbt</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">GradientBoostedTrees</span><span class="o">(</span><span class="n">boostingStrategy</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="c1">// Get class probability</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">treePredictions</span> <span class="k">=</span> <span class="n">test</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">point</span> <span class="k">=&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">trees</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">point</span><span class="o">.</span><span class="n">features</span><span class="o">))</span> <span class="o">}</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">treePredictionsVector</span> <span class="k">=</span> <span class="n">treePredictions</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">array</span> <span class="k">=&gt;</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">array</span><span class="o">))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">treePredictionsMatrix</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">RowMatrix</span><span class="o">(</span><span class="n">treePredictionsVector</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">learningRate</span> <span class="k">=</span> <span class="n">model</span><span class="o">.</span><span class="n">treeWeights</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">learningRateMatrix</span> <span class="k">=</span> <span class="nc">Matrices</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">learningRate</span><span class="o">.</span><span class="n">size</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="n">learningRate</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">weightedTreePredictions</span> <span class="k">=</span> <span class="n">treePredictionsMatrix</span><span class="o">.</span><span class="n">multiply</span><span class="o">(</span><span class="n">learningRateMatrix</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">classProb</span> <span class="k">=</span> <span class="n">weightedTreePredictions</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toArray</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="o">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nc">Math</span><span class="o">.</span><span class="n">exp</span><span class="o">(-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">x</span><span class="o">)))</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">labels</span> <span class="k">=</span> <span class="n">test</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">label</span><span class="o">)</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">predictionAndLabels</span> <span class="k">=</span> <span class="o">(</span><span class="n">classProb</span> <span class="n">zip</span> <span class="n">labels</span><span class="o">).</span><span class="n">persist</span><span class="o">(</span><span class="nc">MEMORY_ONLY</span><span class="o">)</span>
</span><span class='line'>    <span class="k">import</span> <span class="nn">org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">metrics</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">BinaryClassificationMetrics</span><span class="o">(</span><span class="n">predictionAndLabels</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// AUPRC</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">auPRC</span> <span class="k">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">areaUnderPR</span>
</span><span class='line'>    <span class="n">println</span><span class="o">(</span><span class="s">&quot;Area under precision-recall curve = &quot;</span> <span class="o">+</span> <span class="n">auPRC</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// AUROC</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">auROC</span> <span class="k">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">areaUnderROC</span>
</span><span class='line'>    <span class="n">println</span><span class="o">(</span><span class="s">&quot;Area under ROC = &quot;</span> <span class="o">+</span> <span class="n">auROC</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>Model Tuning</em><br/>
GBT in Spark is based on J.H. Friedman&rsquo;s paper in 1999 - &ldquo;Stochastic Gradient Boosting.&rdquo; It trains a series of weakk classifiers, each tree is built on the pseudo residuals of the previous model, and finally the best model that minimizes the loss function will be selected.</p>

<p><strong>categorical features</strong><br/>
Before tuning the model, I set categorical features in <code>boostingStrategy.treeStrategy.categoricalFeaturesInfo</code>, there is some lift immediately:</p>

<table>
<thead>
<tr>
<th>cat. info        </th>
<th>   AUPRC            </th>
</tr>
</thead>
<tbody>
<tr>
<td>yes         </td>
<td>  0.5085462235      </td>
</tr>
<tr>
<td>no          </td>
<td>  0.5094705132      </td>
</tr>
</tbody>
</table>


<p><strong>default configs:</strong><br/>
There are different ways to configure gradient boosting algorithm, this <a href="http://machinelearningmastery.com/configure-gradient-boosting-algorithm/">post</a>, for instance, gives ideas based on different implementations.
Basically there are two types of parameter to be tuned here – tree based and boosting parameters. Default configs are usually linked to internal implementation. In spark, <code>org.apache.spark.mllib.tree.configuration.BoostingStrategy</code> has the following default boosting parameters:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="n">numIterations</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">100</span><span class="o">,</span>
</span><span class='line'> <span class="n">learningRate</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">,</span>
</span><span class='line'> <span class="n">validationTol</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">0.001</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p><strong>learning rate</strong><br/>
Boosting parameters are mostly regularization techniques that reduce overfitting and penalize the complexities of the models. One strategy is giving a large <code>numIterations</code>, and then tune down the shrinkage parameter(or <code>learningRate</code>) to scale the contribution of each weak learner and achieve better result. It comes at the price of increasing training time. Lower learning rate requires more iterations, as rule of thumb for iterations >500, a smaller value of learning rate(&lt;0.1) gives much better generalizetion errors. However in my tests, Spark GBT runs much slower when iterations go up to 200(testing on 400 cpu cores). Below is my test result, clearly an appropriately higher learning rate for small iterations can also lift the scores:</p>

<table>
<thead>
<tr>
<th>Learning Rate  </th>
<th>   AUPRC            </th>
<th> AUROC</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.05           </td>
<td>   0.5030750757     </td>
<td> 0.87685504</td>
</tr>
<tr>
<td>0.1            </td>
<td>   0.5094705132     </td>
<td> 0.8802381335</td>
</tr>
<tr>
<td>0.15           </td>
<td>   0.5095999476     </td>
<td> 0.8811939415</td>
</tr>
</tbody>
</table>


<p><strong>subsampling rate</strong><br/>
Bagging is often used in Random Forest to reduce the variance, it can also be used in GBT(combined with learning rate). By using fraction of samples to fit the individual base learners, it is said to perform better than deterministic  boosting, that&rsquo;s also the idea of &ldquo;Stochastic Gradient Boosting&rdquo;. In the original paper(stochastic boosting), it&rsquo;s suggested subsampling rate around 0.4(or less than 1). However I didn&rsquo;t see this improvement in my test, the reason is probably because the number of iterations is not big enough(subsampling rate often interacts with num iterations):</p>

<table>
<thead>
<tr>
<th>Subsampling Rate  </th>
<th>   AUPRC            </th>
<th> AUROC</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.4               </td>
<td>   0.6965167027     </td>
<td> 0.908289203</td>
</tr>
<tr>
<td>0.8               </td>
<td>   0.6972948047     </td>
<td> 0.9086104967</td>
</tr>
<tr>
<td>1.0               </td>
<td>   0.7056956335     </td>
<td> 0.9113023003</td>
</tr>
</tbody>
</table>


<p><strong>max depth of tree</strong>:<br/>
GBTs generally train shallower trees compared to random forest. They have a smaller variance, and run many iterations to reduce the bias. Similar to RF, I chose the max depth based on number of samples, the more samples the deeper the tree. I used [15,20] for RF, and lowered range for GBT. Below is the testing result based on 10m training samples, clearly deeper trees helped when training large number of data:</p>

<table>
<thead>
<tr>
<th>Max Depth </th>
<th>  Iterations   </th>
<th> AUC-PR         </th>
<th>    AUC-ROC</th>
</tr>
</thead>
<tbody>
<tr>
<td>10        </td>
<td>       100     </td>
<td> 0.7056956335   </td>
<td>  0.9113023003</td>
</tr>
<tr>
<td>12        </td>
<td>       100     </td>
<td>0.7055160758    </td>
<td>  0.9112645385</td>
</tr>
<tr>
<td>12        </td>
<td>       200     </td>
<td>0.7124389723    </td>
<td>  0.9139163337</td>
</tr>
<tr>
<td>13        </td>
<td>       100     </td>
<td>0.7082843826    </td>
<td>  0.9122832517</td>
</tr>
</tbody>
</table>


<p><strong>number of iterations</strong>:<br/>
Increasing number of iterations reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of iterations is often selected by monitoring prediction error on an independent validation data set. In GBT, it not only affects the model quality, but also the training time. I hit the the runtime bottleneck whening increasing number of iterations to 200.</p>

<p>Spark has succesfully eliminates a lot of bottlencks, but introduces other new bottlenecks compared to many single-machine implementations including XGBoost. I captured the timelines from Web UI. At the beginning, schedule delay dominates the run time, clearly this is due to network latency and coordination between executors and drivers. <br/>
<img class="left" src="/images/timeline1.png"></p>

<p>RandomForest:   init: 46.988458552<br/>
  total: 259.506305596<br/>
  findSplits: 22.73275623<br/>
  findBestSplits: 212.461930748<br/>
  chooseSplits: 211.908053401</p>

<p>After a while, task deserialization time is getting longer. The reason is some big data objects are wrapped in the function closure when broadcasting to other executors. One way to improve this is use more cpu cores(e.g. 10 or 20) in each executor, because communication cost between threads is  much smaller than between executors.</p>

<p><img class="right" src="/images/timeline2.png"></p>

<p>RandomForest:   init: 9.169380495<br/>
  total: 100.28455719<br/>
  findSplits: 2.750761187<br/>
  findBestSplits: 91.051591324<br/>
  chooseSplits: 90.761085741</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/02/08/how-to-handle-categorical-features-in-spark-ml/">How to Handle Categorical Features in Spark ML(Random Forest)</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-02-08T15:07:49-08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>8</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>3:07 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Random Forest(RF) is an ensemble training algorithm that is very popular in data science and machine learning communities. It uses bagging and feature subsets to train on 100s of decision trees to reduce overfitting. It&rsquo;s also fast because it&rsquo;s an Embarrassingly parallel task. In production runs using scikit-learn&rsquo;s implementation, it takes less than one hour to train 5 million samples on 30 cpu cores, and several hours if doing a grid search on two grid points with cross validation. Compared to other ensemble algorithms like Gradient Boosted Trees, it&rsquo;s much faster.</p>

<p>RF doesn&rsquo;t require feature normalization, and can handle categorical fetures. However this is up to specific implementations. In Spark ML, it&rsquo;s a little tricky. Before Spark 2.0, MLlib&rsquo;s RDD-based API lets you pass in <code>categoricalFeaturesInfo</code>, which is a mapping from feature index to number of categories, into method:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="nc">RandomForest</span><span class="o">.</span><span class="n">trainClassifier</span><span class="o">(</span><span class="n">trainingData</span><span class="o">,</span> <span class="n">numClasses</span><span class="o">,</span> <span class="n">categoricalFeaturesInfo</span><span class="o">,</span>
</span><span class='line'>  <span class="n">numTrees</span><span class="o">,</span> <span class="n">featureSubsetStrategy</span><span class="o">,</span> <span class="n">impurity</span><span class="o">,</span> <span class="n">maxDepth</span><span class="o">,</span> <span class="n">maxBins</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>However since 2.0 the primary API is the DataFrame-based API in the spark.ml package, which doesn&rsquo;t reach feature parity until 2.2.</p>

<p>There are still several ways to handle categorical fetures. The easiest one is treat all features as continuous variable, e.g. you can do some string splitting on the text file you read in, wrap them in <code>LabeledPoint</code> objects and convert RDD into DataFrame, which is then passed to <code>RandomForestClassifier</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="k">val</span> <span class="n">testDF</span> <span class="k">=</span> <span class="n">testRDD</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">line</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">list</span> <span class="k">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="sc">&#39;,&#39;</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">label</span> <span class="k">=</span> <span class="n">list</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">toDouble</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">features</span> <span class="k">=</span> <span class="n">list</span><span class="o">.</span><span class="n">slice</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">100</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)</span>
</span><span class='line'>      <span class="nc">LabeledPoint</span><span class="o">(</span><span class="n">label</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">features</span><span class="o">))</span>
</span><span class='line'>    <span class="o">}.</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;label&quot;</span><span class="o">,</span> <span class="s">&quot;features&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>However, you do it the easy way at the expense of the model quality. In my testing, it only reaches AUC-PR score of 0.45 compared to 0.49 in scikit-learn.</p>

<p>Another way is build a pipeline that combines feature transformers and model estimators. We can use <code>VectorIndexer</code> to automatically identify categorical features based on threshold for the number of values a categorical feature can take. If a feature is found to have > maxCategories distinct values then it is declared continuous, otherwise it&rsquo;s declared categorical feature. By default it&rsquo;s 20, but you can try different values:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="k">val</span> <span class="n">featureIndexer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">VectorIndexer</span><span class="o">()</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setInputCol</span><span class="o">(</span><span class="s">&quot;features&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setOutputCol</span><span class="o">(</span><span class="s">&quot;indexedFeatures&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">setMaxCategories</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>
</span><span class='line'>      <span class="o">.</span><span class="n">fit</span><span class="o">(</span><span class="n">train</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Everything looks fine up to this point. But when you run model prediction, you immediately see the following error:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">NoSuchElementException</span><span class="k">:</span> <span class="kt">key</span> <span class="kt">not</span> <span class="kt">found:</span> <span class="err">3</span><span class="kt">.</span><span class="err">0</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.MapLike$class.default</span><span class="o">(</span><span class="kt">MapLike.scala:</span><span class="err">228</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.AbstractMap.default</span><span class="o">(</span><span class="kt">Map.scala:</span><span class="err">58</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.MapLike$class.apply</span><span class="o">(</span><span class="kt">MapLike.scala:</span><span class="err">141</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.AbstractMap.apply</span><span class="o">(</span><span class="kt">Map.scala:</span><span class="err">58</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply</span><span class="o">(</span><span class="kt">VectorIndexer.scala:</span><span class="err">316</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10$$anonfun$apply$4.apply</span><span class="o">(</span><span class="kt">VectorIndexer.scala:</span><span class="err">315</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.immutable.HashMap$HashMap1.foreach</span><span class="o">(</span><span class="kt">HashMap.scala:</span><span class="err">224</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.immutable.HashMap$HashTrieMap.foreach</span><span class="o">(</span><span class="kt">HashMap.scala:</span><span class="err">403</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">scala.collection.immutable.HashMap$HashTrieMap.foreach</span><span class="o">(</span><span class="kt">HashMap.scala:</span><span class="err">403</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply</span><span class="o">(</span><span class="kt">VectorIndexer.scala:</span><span class="err">315</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$10.apply</span><span class="o">(</span><span class="kt">VectorIndexer.scala:</span><span class="err">309</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply</span><span class="o">(</span><span class="kt">VectorIndexer.scala:</span><span class="err">351</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.ml.feature.VectorIndexerModel$$anonfun$11.apply</span><span class="o">(</span><span class="kt">VectorIndexer.scala:</span><span class="err">351</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">at</span> <span class="kt">org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply</span><span class="o">(</span><span class="kt">Unknown</span> <span class="kt">Source</span><span class="o">)</span>
</span><span class='line'>  <span class="kt">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>This is caused by unknown categorical features that are not in VectorIndexer&rsquo;s map(<a href="https://issues.apache.org/jira/browse/SPARK-12375">SPARK-12375</a>). The issue is not resolved yet, but you can try installing the patch.</p>

<p>You might wonder, is there a way to pass in category info into the DataFrame-based model? The answer is, sort of. Since Spark 1.2, there is metadata field(<a href="https://issues.apache.org/jira/browse/SPARK-3569">SPARK-3569</a>) in the schema that can be used by machine learning applications to store information like categorical/continuous, number categories, category-to-index map. It&rsquo;s keyed by &ldquo;ml_attr&rdquo;. This field is empty by default, it&rsquo;ll generate something for you if you run VectorIndexer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">train</span><span class="o">.</span><span class="n">schema</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">metadata</span>
</span><span class='line'><span class="n">res6</span><span class="k">:</span> <span class="kt">org.apache.spark.sql.</span><span class="k">type</span><span class="kt">s.Metadata</span> <span class="o">=</span> <span class="o">{}</span>
</span><span class='line'><span class="n">scala</span><span class="o">&gt;</span> <span class="n">field</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">getMetadata</span><span class="o">(</span><span class="s">&quot;ml_attr&quot;</span><span class="o">)</span>
</span><span class='line'><span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">NoSuchElementException</span><span class="k">:</span> <span class="kt">key</span> <span class="kt">not</span> <span class="kt">found:</span> <span class="kt">ml_attr</span>
</span><span class='line'><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>RandomForestClassifier.scala</code> then checks the metadata field to get the list of categorical information:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>    <span class="k">val</span> <span class="n">categoricalFeatures</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">]</span> <span class="k">=</span>
</span><span class='line'>      <span class="nc">MetadataUtils</span><span class="o">.</span><span class="n">getCategoricalFeatures</span><span class="o">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">schema</span><span class="o">(</span><span class="n">$</span><span class="o">(</span><span class="n">featuresCol</span><span class="o">)))</span>
</span></code></pre></td></tr></table></div></figure>


<p>If we dig a little deeper to see how the mapping is generated, there is an object called <code>NominalAttribute</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>          <span class="n">attr</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>            <span class="k">case</span> <span class="k">_:</span> <span class="kt">NumericAttribute</span> <span class="kt">|</span> <span class="kt">UnresolvedAttribute</span> <span class="o">=&gt;</span> <span class="nc">Iterator</span><span class="o">()</span>
</span><span class='line'>            <span class="k">case</span> <span class="n">binAttr</span><span class="k">:</span> <span class="kt">BinaryAttribute</span> <span class="o">=&gt;</span> <span class="nc">Iterator</span><span class="o">(</span><span class="n">idx</span> <span class="o">-&gt;</span> <span class="mi">2</span><span class="o">)</span>
</span><span class='line'>            <span class="k">case</span> <span class="n">nomAttr</span><span class="k">:</span> <span class="kt">NominalAttribute</span> <span class="o">=&gt;</span>
</span><span class='line'>              <span class="n">nomAttr</span><span class="o">.</span><span class="n">getNumValues</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>                <span class="k">case</span> <span class="nc">Some</span><span class="o">(</span><span class="n">numValues</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="nc">Iterator</span><span class="o">(</span><span class="n">idx</span> <span class="o">-&gt;</span> <span class="n">numValues</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The bad news is there is no public API yet(<a href="https://issues.apache.org/jira/browse/SPARK-8515">SPARK-8515</a>) to generate an <code>NominalAttribute</code> object. The only factory method looks like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="k">object</span> <span class="nc">NominalAttribute</span> <span class="k">extends</span> <span class="nc">AttributeFactory</span> <span class="o">{</span>
</span><span class='line'>     <span class="cm">/** The default nominal attribute. */</span>
</span><span class='line'>     <span class="k">final</span> <span class="k">val</span> <span class="n">defaultAttr</span><span class="k">:</span> <span class="kt">NominalAttribute</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">NominalAttribute</span>
</span><span class='line'>     <span class="k">private</span><span class="o">[</span><span class="kt">attribute</span><span class="o">]</span> <span class="k">override</span> <span class="k">def</span> <span class="n">fromMetadata</span><span class="o">(</span><span class="n">metadata</span><span class="k">:</span> <span class="kt">Metadata</span><span class="o">)</span><span class="k">:</span> <span class="kt">NominalAttribute</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="o">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>Of course, you can always hack the core files and build your Spark. Have fun!</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/01/08/bloom-filter-join-in-spark/">Bloom-filter Join in Spark</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-01-08T14:33:32-08:00'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>8</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>2:33 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>When it comes to join optimization techniques, map-side(broadcast) join is an obvious candidate. However it doesn&rsquo;t work when the data set is not small enough to fit into memory. But we can extend this approach if we find a representation of the data set that is small, e.g. a Bloom filter.</p>

<p>A Bloom filter is a space-efficient probabilistic data structure used to test whether a member is an element of a set(&lt;10 bits per element are required for a 1% false positive rate). Recently I ported a job from Apache Pig to Spark which gained significant speedup by using Bloom filter. This job joins 60 days of mobile-device pairs to cookies from a partner.</p>

<p>The optimized join goes like this: first build partial Bloom filters in each partition of smaller data, which can be done using <code>mapPartitions</code> in Spark. They are collected into driver node and merged into a full Bloom filter, which is then distributed to all executors and used to filter out large portions of the data that will not find a match when joined.</p>

<p>Implementation of Bloom filter in MapReduce is cumbersome in that you have to explicitly use <code>DistributedCache</code>, and write a lot of boilerplate code that handles file system I/O when writing it to HDFS and read it back later. Mapper(container) with only one cpu core is also inefficient when the filter is large. While <code>MultithreadedMapper</code> is possible, it&rsquo;s difficult to write code that is thread-safe. Spark&rsquo;s executor is a thread-pool by design, so you can easily assign many cpu cores to an executor. And Spark has a nice feature called Broadcast Variables that saves you a lot of effort, allowing you to distribute large data using efficient BitTorrent-like algorithms to reduce communication cost.</p>

<p>The good news is we don&rsquo;t need to write our own Bloom filter from scratch, instead we can use <code>org.apache.spark.util.sketch.BloomFilter</code> that is largely based on Google&rsquo;s Guava library. Under the hood it&rsquo;s a <code>long[]</code> representing a bit array, it has the advantage over other implementation in that the number of inserted bits can be larger than 4bn.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">org.apache.spark.util.sketch.BloomFilter</span>
</span><span class='line'>
</span><span class='line'><span class="c1">//reading files</span>
</span><span class='line'><span class="k">val</span> <span class="n">bigRDD</span> <span class="k">=</span> <span class="o">...</span>
</span><span class='line'><span class="k">val</span> <span class="n">smallRDD</span> <span class="k">=</span> <span class="o">...</span>
</span><span class='line'><span class="k">val</span> <span class="n">cnt</span><span class="k">:</span><span class="kt">Long</span> <span class="o">=</span> <span class="n">smallRDD</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// 1a. create bloom filters for smaller data locally on each partition</span>
</span><span class='line'><span class="c1">// 1b. merge them in driver</span>
</span><span class='line'><span class="k">val</span> <span class="n">bf</span> <span class="k">=</span> <span class="n">smallRDD</span><span class="o">.</span><span class="n">mapPartitions</span> <span class="o">{</span> <span class="n">iter</span> <span class="k">=&gt;</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">b</span> <span class="k">=</span> <span class="nc">BloomFilter</span><span class="o">.</span><span class="n">create</span><span class="o">(</span><span class="n">cnt</span><span class="o">,</span> <span class="mf">0.1</span><span class="o">)</span>  <span class="c1">//false positive probability=0.1</span>
</span><span class='line'>  <span class="n">iter</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="n">b</span><span class="o">.</span><span class="n">putString</span><span class="o">(</span><span class="n">i</span><span class="o">.</span><span class="n">_1</span><span class="o">))</span>
</span><span class='line'>  <span class="nc">Iterator</span><span class="o">(</span><span class="n">b</span><span class="o">)</span>
</span><span class='line'><span class="o">}.</span><span class="n">reduce</span><span class="o">((</span><span class="n">x</span><span class="o">,</span><span class="n">y</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">mergeInPlace</span><span class="o">(</span><span class="n">y</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// 2. driver broadcasts bloom-filter</span>
</span><span class='line'><span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="o">(</span><span class="n">bf</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// 3. use bloom-filter to filter big data set</span>
</span><span class='line'><span class="k">val</span> <span class="n">filtered</span><span class="k">=</span> <span class="n">bigRDD</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">bf</span><span class="o">.</span><span class="n">mightContain</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">bf</span><span class="o">.</span><span class="n">mightContain</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// 4. join big data set and small data set</span>
</span><span class='line'><span class="n">filtered</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">smallRDD</span><span class="o">).</span><span class="n">saveAsTextFile</span><span class="o">(</span><span class="n">args</span><span class="o">(</span><span class="mi">2</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>In order for this to run on the Hadoop cluster, we need to set sufficiently large memory on both driver and executors to hold the underlying bit array, depending on the value of false positive probability we&rsquo;ve set above. And we need to increase the maximum allowable size of Kryo serialization buffer, otherwise we&rsquo;ll see exceptions from Kryo:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">spark</span><span class="o">.</span><span class="n">kryoserializer</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">max</span>  <span class="mi">512</span><span class="n">m</span>
</span><span class='line'> <span class="o">--</span><span class="n">driver</span><span class="o">-</span><span class="n">memory</span> <span class="mi">100</span><span class="n">g</span> <span class="o">\</span>
</span><span class='line'> <span class="o">--</span><span class="n">driver</span><span class="o">-</span><span class="n">cores</span> <span class="mi">24</span> <span class="o">\</span>
</span><span class='line'> <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">executors</span> <span class="mi">130</span> <span class="o">\</span>
</span><span class='line'> <span class="o">--</span><span class="n">executor</span><span class="o">-</span><span class="n">memory</span> <span class="mi">50</span><span class="n">g</span> <span class="o">\</span>
</span><span class='line'> <span class="o">--</span><span class="n">executor</span><span class="o">-</span><span class="n">cores</span> <span class="mi">8</span> <span class="o">\</span>
</span></code></pre></td></tr></table></div></figure>


<p>I ran a test on the job I mentioned above. Simply counting the number of records in each stage shows >90% savings in shuffle size!</p>

<pre><code>Small data set: 244,071,770 records
Big data set: 42,504,945,562 records
After filtered: 1,587,344,750 records
After joined: 697,421,722 records
</code></pre>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/04/10/useful-configs-for-spark-in-production/">Useful Configs for Spark in Production</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/02/27/how-to-tune-gradient-boosted-trees-in-spark/">How to Tune Gradient-Boosted Trees in Spark</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/02/08/how-to-handle-categorical-features-in-spark-ml/">How to Handle Categorical Features in Spark ML(Random Forest)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/08/bloom-filter-join-in-spark/">Bloom-filter Join in Spark</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Zhuo Tao -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
